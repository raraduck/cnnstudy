{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16623c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: nibabel in d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages (from nibabel) (1.21.6)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages (from nibabel) (23.1)\n",
      "Requirement already satisfied: setuptools in d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages (from nibabel) (47.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install SimpleITK nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ef92b4-bb0b-4739-b0e4-54c45655ea4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\dwm\\jupyter_basic\\ch10_monai\\downloaded_data\\data.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from configparser import ConfigParser\n",
    "import os\n",
    "\n",
    "def get_data_url_from_model_zoo():\n",
    "    url = 'https://raw.githubusercontent.com/NifTK/NiftyNetModelZoo/5-reorganising-with-lfs/highres3dnet_brain_parcellation/main.ini'\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        config_string = response.read().decode()\n",
    "    config = ConfigParser()\n",
    "    config.read_string(config_string)\n",
    "    data_url = config['data']['url']\n",
    "    return data_url\n",
    "\n",
    "\n",
    "def download_data(data_url):\n",
    "    # tempdir = Path(tempfile.gettempdir())\n",
    "    tempdir = Path(os.getcwd())\n",
    "    download_dir = tempdir / 'downloaded_data'\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    data_path = download_dir / Path(data_url).name\n",
    "    print(data_path)\n",
    "    if not data_path.is_file():\n",
    "        urllib.request.urlretrieve(data_url, data_path)\n",
    "    with tarfile.open(data_path, 'r') as tar:\n",
    "        tar.extractall(download_dir)\n",
    "    nifti_files = download_dir.glob('**/*.nii.gz')\n",
    "    return list(nifti_files)[0]\n",
    "\n",
    "\n",
    "def test_infer():\n",
    "    image_path = download_data(get_data_url_from_model_zoo())\n",
    "\n",
    "test_infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a723ad54-3f57-45c3-9a7d-afb8f3252248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0da670-9f98-43d4-be9a-a2d65919bc13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    HighRes3DNet by Li et al. 2017 for T1-MRI brain parcellation\n",
      "    pretrained (bool): load parameters from pretrained model\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Daewoon/.cache\\torch\\hub\\fepegar_highresnet_master\n",
      "Using cache found in C:\\Users\\Daewoon/.cache\\torch\\hub\\fepegar_highresnet_master\n"
     ]
    }
   ],
   "source": [
    "repo = 'fepegar/highresnet'\n",
    "model_name = 'highres3dnet'\n",
    "print(torch.hub.help(repo, model_name))\n",
    "\"HighRes3DNet by Li et al. 2017 for T1-MRI brain parcellation\"\n",
    "\"pretrained (bool): load parameters from pretrained model\"\n",
    "model = torch.hub.load(repo, model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71ef3acd-7701-4da8-94eb-ab565b4d1176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def resample_spacing(nifti, output_spacing, interpolation):\n",
    "    output_spacing = tuple(output_spacing)\n",
    "    temp_dir = Path(tempfile.gettempdir()) / '.deepgif'\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    temp_path = temp_dir / 'deepgif_resampled.nii'\n",
    "    temp_path = str(temp_path)\n",
    "\n",
    "    nifti.to_filename(temp_path)\n",
    "    image = sitk.ReadImage(temp_path)\n",
    "\n",
    "    output_spacing = np.array(output_spacing).astype(float)\n",
    "    output_spacing = tuple(output_spacing)\n",
    "\n",
    "    reference_spacing = np.array(image.GetSpacing())\n",
    "    reference_size = np.array(image.GetSize())\n",
    "\n",
    "    output_size = reference_spacing / output_spacing * reference_size\n",
    "    output_size = np.round(output_size).astype(np.uint32)\n",
    "    # tuple(output_size) does not work, see\n",
    "    # https://github.com/Radiomics/pyradiomics/issues/204\n",
    "    output_size = output_size.tolist()\n",
    "\n",
    "    identity = sitk.Transform(3, sitk.sitkIdentity)\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetInterpolator(interpolation)\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(image.GetOrigin())  # TODO: double-check that this is correct\n",
    "    resample.SetOutputPixelType(image.GetPixelID())\n",
    "    resample.SetOutputSpacing(output_spacing)\n",
    "    resample.SetSize(output_size)\n",
    "    resample.SetTransform(identity)\n",
    "    resampled = resample.Execute(image)\n",
    "    sitk.WriteImage(resampled, temp_path)\n",
    "    nifti_resampled = nib.load(temp_path)\n",
    "    return nifti_resampled\n",
    "    \n",
    "def check_header(nifti_image):\n",
    "    orientation = ''.join(nib.aff2axcodes(nifti_image.affine))\n",
    "    spacing = nifti_image.header.get_zooms()[:3]\n",
    "    one_iso = 1, 1, 1\n",
    "    # print(f'spacing and one_iso: {spacing} {one_iso}')\n",
    "    is_ras = orientation == 'RAS'\n",
    "    if not is_ras:\n",
    "        print(f'Detected orientation: {orientation}. Reorienting to RAS...')\n",
    "    is_1_iso = np.allclose(spacing, one_iso)\n",
    "    if not is_1_iso:\n",
    "        print(f'Detected spacing: {spacing}. Resampling to 1 mm iso...')\n",
    "    needs_resampling = not is_ras or not is_1_iso\n",
    "    return needs_resampling\n",
    "\n",
    "def resample_ras_1mm_iso(nifti, interpolation=None):\n",
    "    if interpolation is None:\n",
    "        interpolation = sitk.sitkLinear\n",
    "    nii_ras = nib.as_closest_canonical(nifti)\n",
    "    spacing = nii_ras.header.get_zooms()[:3]\n",
    "    one_iso = 1, 1, 1\n",
    "    if np.allclose(spacing, one_iso):\n",
    "        return nii_ras\n",
    "    nii_resampled = resample_spacing(\n",
    "        nii_ras,\n",
    "        output_spacing=one_iso,\n",
    "        interpolation=interpolation,\n",
    "    )\n",
    "    return nii_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27af9c12-b1d9-4dbd-8bd3-f570e7dd106b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "def mean_plus(data):\n",
    "    return data > data.mean()\n",
    "\n",
    "def whiten(data, masking_function=None):\n",
    "    if masking_function is None:\n",
    "        masking_function = mean_plus\n",
    "    mask_data = masking_function(data)\n",
    "    values = data[mask_data]\n",
    "    mean, std = values.mean(), values.std()\n",
    "    data -= mean\n",
    "    data /= std\n",
    "    return data\n",
    "\n",
    "def __compute_percentiles(img, mask, cutoff):\n",
    "    \"\"\"\n",
    "    Creates the list of percentile values to be used as landmarks for the\n",
    "    linear fitting.\n",
    "\n",
    "    :param img: Image on which to determine the percentiles\n",
    "    :param mask: Mask to use over the image to constraint to the relevant\n",
    "    information\n",
    "    :param cutoff: Values of the minimum and maximum percentiles to use for\n",
    "    the linear fitting\n",
    "    :return perc_results: list of percentiles value for the given image over\n",
    "    the mask\n",
    "    \"\"\"\n",
    "    perc = [cutoff[0],\n",
    "            0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9,\n",
    "            cutoff[1]]\n",
    "    masked_img = ma.masked_array(img, np.logical_not(mask)).compressed()\n",
    "    perc_results = np.percentile(masked_img, 100 * np.array(perc))\n",
    "    return perc_results\n",
    "\n",
    "def __standardise_cutoff(cutoff, type_hist='percentile'):\n",
    "    \"\"\"\n",
    "    Standardises the cutoff values given in the configuration\n",
    "\n",
    "    :param cutoff:\n",
    "    :param type_hist: Type of landmark normalisation chosen (median,\n",
    "    quartile, percentile)\n",
    "    :return cutoff: cutoff with appropriate adapted values\n",
    "    \"\"\"\n",
    "    cutoff = np.asarray(cutoff)\n",
    "    if cutoff is None:\n",
    "        return DEFAULT_CUTOFF\n",
    "    if len(cutoff) > 2:\n",
    "        cutoff = np.unique([np.min(cutoff), np.max(cutoff)])\n",
    "    if len(cutoff) < 2:\n",
    "        return DEFAULT_CUTOFF\n",
    "    if cutoff[0] > cutoff[1]:\n",
    "        cutoff[0], cutoff[1] = cutoff[1], cutoff[0]\n",
    "    cutoff[0] = max(0., cutoff[0])\n",
    "    cutoff[1] = min(1., cutoff[1])\n",
    "    if type_hist == 'quartile':\n",
    "        cutoff[0] = np.min([cutoff[0], 0.24])\n",
    "        cutoff[1] = np.max([cutoff[1], 0.76])\n",
    "    else:\n",
    "        cutoff[0] = np.min([cutoff[0], 0.09])\n",
    "        cutoff[1] = np.max([cutoff[1], 0.91])\n",
    "    return cutoff\n",
    "\n",
    "DEFAULT_CUTOFF = (0.01, 0.99)\n",
    "\n",
    "def normalize(data, landmarks, cutoff=DEFAULT_CUTOFF, masking_function=None):\n",
    "    mapping = landmarks\n",
    "\n",
    "    img = data\n",
    "    image_shape = img.shape\n",
    "    img = img.reshape(-1).astype(np.float32)\n",
    "\n",
    "    if masking_function is not None:\n",
    "        mask = masking_function(img)\n",
    "    else:\n",
    "        mask = np.ones_like(img, dtype=np.bool)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    range_to_use = [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12]\n",
    "\n",
    "    cutoff = __standardise_cutoff(cutoff)\n",
    "    perc = __compute_percentiles(img, mask, cutoff)\n",
    "\n",
    "    # Apply linear histogram standardisation\n",
    "    range_mapping = mapping[range_to_use]\n",
    "    range_perc = perc[range_to_use]\n",
    "    diff_mapping = range_mapping[1:] - range_mapping[:-1]\n",
    "    diff_perc = range_perc[1:] - range_perc[:-1]\n",
    "\n",
    "    # handling the case where two landmarks are the same\n",
    "    # for a given input image. This usually happens when\n",
    "    # image background is not removed from the image.\n",
    "    diff_perc[diff_perc == 0] = np.inf\n",
    "\n",
    "    affine_map = np.zeros([2, len(range_to_use) - 1])\n",
    "    # compute slopes of the linear models\n",
    "    affine_map[0] = diff_mapping / diff_perc\n",
    "    # compute intercepts of the linear models\n",
    "    affine_map[1] = range_mapping[:-1] - affine_map[0] * range_perc[:-1]\n",
    "\n",
    "    bin_id = np.digitize(img, range_perc[1:-1], right=False)\n",
    "    lin_img = affine_map[0, bin_id]\n",
    "    aff_img = affine_map[1, bin_id]\n",
    "    new_img = lin_img * img + aff_img\n",
    "    new_img = new_img.reshape(image_shape)\n",
    "\n",
    "    return new_img\n",
    "\n",
    "\n",
    "# From NiftyNet model zoo\n",
    "LI_LANDMARKS = \"4.4408920985e-16 8.06305571158 15.5085721044 18.7007018006 21.5032879029 26.1413278906 29.9862059045 33.8384058795 38.1891334787 40.7217966068 44.0109152758 58.3906435207 100.0\"\n",
    "LI_LANDMARKS = np.array([float(n) for n in LI_LANDMARKS.split()])\n",
    "\n",
    "def standardize(data, landmarks=LI_LANDMARKS, masking_function=None):\n",
    "    return normalize(data, landmarks, masking_function=masking_function)\n",
    "\n",
    "def pad(data, padding):\n",
    "    # Should I use this value for padding?\n",
    "    value = data[0, 0, 0]\n",
    "    return np.pad(data, padding, mode='constant', constant_values=value)\n",
    "\n",
    "def preprocess(data, padding, hist_masking_function=None):\n",
    "    # data = pad(data, padding)\n",
    "    data = standardize(data, masking_function=hist_masking_function)\n",
    "    data = whiten(data)\n",
    "    data = data.astype(np.float32)\n",
    "    data = pad(data, padding)  # should I pad at the beginning instead?\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e67b4a4-316b-48da-b81e-d3abdeafd1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_path1 = \"_data/GAAIN/AD01_MR.nii\"\n",
    "# nii1 = nib.load(str(input_path1))\n",
    "input_path2 = \"_data/OASIS/OAS1_MR.nii.gz\"\n",
    "input_path3 = \"_data/SNU/P01_MR.nii\"\n",
    "# nii2 = nib.load(str(input_path2))\n",
    "# print(nii1)\n",
    "# print(nii2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4be09f18-e487-407b-a67f-97ddac836332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected orientation: ASL. Reorienting to RAS...\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_path = input_path2\n",
    "nii = nib.load(str(input_path))\n",
    "needs_resampling = check_header(nii)\n",
    "print(needs_resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "411cee67-947c-46fc-8d8c-9d8faa4d0ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if needs_resampling:\n",
    "    nii = resample_ras_1mm_iso(nii)\n",
    "data = nii.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8313f76-a871-4f1e-a1c7-ded896e8587a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages\\ipykernel_launcher.py:76: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "use_niftynet_hist_std = False\n",
    "# Preprocessing\n",
    "hist_masking_function = mean_plus if use_niftynet_hist_std else None\n",
    "volume_padding = int(10)\n",
    "preprocessed = preprocess(\n",
    "    data,\n",
    "    volume_padding,\n",
    "    hist_masking_function=hist_masking_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa5d27-150d-4626-bc0c-4a188aaa5782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "77a35ccee26993ab16be666979a7bc0119dbfe0fa7b2d59f42b1045260f18305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
