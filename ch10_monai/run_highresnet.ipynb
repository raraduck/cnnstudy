{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c210014d-c804-4a1c-9a00-4472059c4f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dac5c-80ae-43bc-9155-fa5d1b544fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# histogram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e9eeaf8-fcf2-4611-81c9-5e45282006af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "\n",
    "DEFAULT_CUTOFF = (0.01, 0.99)\n",
    "\n",
    "\n",
    "# Functions from NiftyNet\n",
    "\n",
    "def __compute_percentiles(img, mask, cutoff):\n",
    "    \"\"\"\n",
    "    Creates the list of percentile values to be used as landmarks for the\n",
    "    linear fitting.\n",
    "\n",
    "    :param img: Image on which to determine the percentiles\n",
    "    :param mask: Mask to use over the image to constraint to the relevant\n",
    "    information\n",
    "    :param cutoff: Values of the minimum and maximum percentiles to use for\n",
    "    the linear fitting\n",
    "    :return perc_results: list of percentiles value for the given image over\n",
    "    the mask\n",
    "    \"\"\"\n",
    "    perc = [cutoff[0],\n",
    "            0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9,\n",
    "            cutoff[1]]\n",
    "    masked_img = ma.masked_array(img, np.logical_not(mask)).compressed()\n",
    "    perc_results = np.percentile(masked_img, 100 * np.array(perc))\n",
    "    return perc_results\n",
    "\n",
    "\n",
    "def __standardise_cutoff(cutoff, type_hist='percentile'):\n",
    "    \"\"\"\n",
    "    Standardises the cutoff values given in the configuration\n",
    "\n",
    "    :param cutoff:\n",
    "    :param type_hist: Type of landmark normalisation chosen (median,\n",
    "    quartile, percentile)\n",
    "    :return cutoff: cutoff with appropriate adapted values\n",
    "    \"\"\"\n",
    "    cutoff = np.asarray(cutoff)\n",
    "    if cutoff is None:\n",
    "        return DEFAULT_CUTOFF\n",
    "    if len(cutoff) > 2:\n",
    "        cutoff = np.unique([np.min(cutoff), np.max(cutoff)])\n",
    "    if len(cutoff) < 2:\n",
    "        return DEFAULT_CUTOFF\n",
    "    if cutoff[0] > cutoff[1]:\n",
    "        cutoff[0], cutoff[1] = cutoff[1], cutoff[0]\n",
    "    cutoff[0] = max(0., cutoff[0])\n",
    "    cutoff[1] = min(1., cutoff[1])\n",
    "    if type_hist == 'quartile':\n",
    "        cutoff[0] = np.min([cutoff[0], 0.24])\n",
    "        cutoff[1] = np.max([cutoff[1], 0.76])\n",
    "    else:\n",
    "        cutoff[0] = np.min([cutoff[0], 0.09])\n",
    "        cutoff[1] = np.max([cutoff[1], 0.91])\n",
    "    return cutoff\n",
    "\n",
    "\n",
    "def create_standard_range():\n",
    "    return 0., 100.\n",
    "\n",
    "\n",
    "def __averaged_mapping(perc_database, s1, s2):\n",
    "    \"\"\"\n",
    "    Map the landmarks of the database to the chosen range\n",
    "    :param perc_database: perc_database over which to perform the averaging\n",
    "    :param s1, s2: limits of the mapping range\n",
    "    :return final_map: the average mapping\n",
    "    \"\"\"\n",
    "    # assuming shape: n_data_points = perc_database.shape[0]\n",
    "    #                 n_percentiles = perc_database.shape[1]\n",
    "    slope = (s2 - s1) / (perc_database[:, -1] - perc_database[:, 0])\n",
    "    slope = np.nan_to_num(slope)\n",
    "    final_map = slope.dot(perc_database) / perc_database.shape[0]\n",
    "    intercept = np.mean(s1 - slope * perc_database[:, 0])\n",
    "    final_map = final_map + intercept\n",
    "    return final_map\n",
    "\n",
    "\n",
    "def normalize(data, landmarks, cutoff=DEFAULT_CUTOFF, masking_function=None):\n",
    "    mapping = landmarks\n",
    "\n",
    "    img = data\n",
    "    image_shape = img.shape\n",
    "    img = img.reshape(-1).astype(np.float32)\n",
    "\n",
    "    if masking_function is not None:\n",
    "        mask = masking_function(img)\n",
    "    else:\n",
    "        mask = np.ones_like(img, dtype=np.bool)\n",
    "    mask = mask.reshape(-1)\n",
    "\n",
    "    range_to_use = [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12]\n",
    "\n",
    "    cutoff = __standardise_cutoff(cutoff)\n",
    "    perc = __compute_percentiles(img, mask, cutoff)\n",
    "\n",
    "    # Apply linear histogram standardisation\n",
    "    range_mapping = mapping[range_to_use]\n",
    "    range_perc = perc[range_to_use]\n",
    "    diff_mapping = range_mapping[1:] - range_mapping[:-1]\n",
    "    diff_perc = range_perc[1:] - range_perc[:-1]\n",
    "\n",
    "    # handling the case where two landmarks are the same\n",
    "    # for a given input image. This usually happens when\n",
    "    # image background is not removed from the image.\n",
    "    diff_perc[diff_perc == 0] = np.inf\n",
    "\n",
    "    affine_map = np.zeros([2, len(range_to_use) - 1])\n",
    "    # compute slopes of the linear models\n",
    "    affine_map[0] = diff_mapping / diff_perc\n",
    "    # compute intercepts of the linear models\n",
    "    affine_map[1] = range_mapping[:-1] - affine_map[0] * range_perc[:-1]\n",
    "\n",
    "    bin_id = np.digitize(img, range_perc[1:-1], right=False)\n",
    "    lin_img = affine_map[0, bin_id]\n",
    "    aff_img = affine_map[1, bin_id]\n",
    "    new_img = lin_img * img + aff_img\n",
    "    new_img = new_img.reshape(image_shape)\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a60d28-6633-4865-a426-aefd0d4be02d",
   "metadata": {},
   "source": [
    "# preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b349f9fa-a9dd-4f98-b277-7c0cedaadc46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# From NiftyNet model zoo\n",
    "LI_LANDMARKS = \"4.4408920985e-16 8.06305571158 15.5085721044 18.7007018006 21.5032879029 26.1413278906 29.9862059045 33.8384058795 38.1891334787 40.7217966068 44.0109152758 58.3906435207 100.0\"\n",
    "LI_LANDMARKS = np.array([float(n) for n in LI_LANDMARKS.split()])\n",
    "\n",
    "\n",
    "def preprocess(data, padding, hist_masking_function=None):\n",
    "    # data = pad(data, padding)\n",
    "    data = standardize(data, masking_function=hist_masking_function)\n",
    "    data = whiten(data)\n",
    "    data = data.astype(np.float32)\n",
    "    data = pad(data, padding)  # should I pad at the beginning instead?\n",
    "    return data\n",
    "\n",
    "\n",
    "def pad(data, padding):\n",
    "    # Should I use this value for padding?\n",
    "    value = data[0, 0, 0]\n",
    "    return np.pad(data, padding, mode='constant', constant_values=value)\n",
    "\n",
    "\n",
    "def crop(data, padding):\n",
    "    p = padding\n",
    "    return data[p:-p, p:-p, p:-p]\n",
    "\n",
    "\n",
    "def standardize(data, landmarks=LI_LANDMARKS, masking_function=None):\n",
    "    return normalize(data, landmarks, masking_function=masking_function)\n",
    "\n",
    "\n",
    "def whiten(data, masking_function=None):\n",
    "    if masking_function is None:\n",
    "        masking_function = mean_plus\n",
    "    mask_data = masking_function(data)\n",
    "    values = data[mask_data]\n",
    "    mean, std = values.mean(), values.std()\n",
    "    data -= mean\n",
    "    data /= std\n",
    "    return data\n",
    "\n",
    "\n",
    "def mean_plus(data):\n",
    "    return data > data.mean()\n",
    "\n",
    "\n",
    "def resample_spacing(nifti, output_spacing, interpolation):\n",
    "    output_spacing = tuple(output_spacing)\n",
    "    temp_dir = Path(tempfile.gettempdir()) / '.deepgif'\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    temp_path = temp_dir / 'deepgif_resampled.nii'\n",
    "    temp_path = str(temp_path)\n",
    "\n",
    "    nifti.to_filename(temp_path)\n",
    "    image = sitk.ReadImage(temp_path)\n",
    "\n",
    "    output_spacing = np.array(output_spacing).astype(float)\n",
    "    output_spacing = tuple(output_spacing)\n",
    "\n",
    "    reference_spacing = np.array(image.GetSpacing())\n",
    "    reference_size = np.array(image.GetSize())\n",
    "\n",
    "    output_size = reference_spacing / output_spacing * reference_size\n",
    "    output_size = np.round(output_size).astype(np.uint32)\n",
    "    # tuple(output_size) does not work, see\n",
    "    # https://github.com/Radiomics/pyradiomics/issues/204\n",
    "    output_size = output_size.tolist()\n",
    "\n",
    "    identity = sitk.Transform(3, sitk.sitkIdentity)\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetInterpolator(interpolation)\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(image.GetOrigin())  # TODO: double-check that this is correct\n",
    "    resample.SetOutputPixelType(image.GetPixelID())\n",
    "    resample.SetOutputSpacing(output_spacing)\n",
    "    resample.SetSize(output_size)\n",
    "    resample.SetTransform(identity)\n",
    "    resampled = resample.Execute(image)\n",
    "    sitk.WriteImage(resampled, temp_path)\n",
    "    nifti_resampled = nib.load(temp_path)\n",
    "    return nifti_resampled\n",
    "\n",
    "\n",
    "def resample_ras_1mm_iso(nifti, interpolation=None):\n",
    "    if interpolation is None:\n",
    "        interpolation = sitk.sitkLinear\n",
    "    nii_ras = nib.as_closest_canonical(nifti)\n",
    "    spacing = nii_ras.header.get_zooms()[:3]\n",
    "    one_iso = 1, 1, 1\n",
    "    if np.allclose(spacing, one_iso):\n",
    "        return nii_ras\n",
    "    nii_resampled = resample_spacing(\n",
    "        nii_ras,\n",
    "        output_spacing=one_iso,\n",
    "        interpolation=interpolation,\n",
    "    )\n",
    "    return nii_resampled\n",
    "\n",
    "\n",
    "def resample_to_reference(\n",
    "        reference_path,\n",
    "        floating_path,\n",
    "        result_path,\n",
    "        interpolation=None,\n",
    "        default_value=0.0,\n",
    "        ):\n",
    "    if interpolation is None:\n",
    "        interpolation = sitk.sitkNearestNeighbor\n",
    "    reference = sitk.ReadImage(str(reference_path))\n",
    "    floating = sitk.ReadImage(str(floating_path))\n",
    "    transform = sitk.Transform(3, sitk.sitkIdentity)\n",
    "    resampled = sitk.Resample(\n",
    "        floating,\n",
    "        reference,\n",
    "        transform,\n",
    "        interpolation,\n",
    "        default_value,\n",
    "        floating.GetPixelID(),\n",
    "    )\n",
    "    sitk.WriteImage(resampled, str(result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873576f7-1923-4887-be47-475925e0d785",
   "metadata": {},
   "source": [
    "# sampling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af0733f-3d4b-4ca4-8147-09a9130cb6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GridSampler(Dataset):\n",
    "    \"\"\"\n",
    "    Adapted from NiftyNet\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window_size, border):\n",
    "        self.array = data\n",
    "        self.locations = self.grid_spatial_coordinates(\n",
    "            self.array,\n",
    "            window_size,\n",
    "            border,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.locations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Assume 3D\n",
    "        location = self.locations[index]\n",
    "        i_ini, j_ini, k_ini, i_fin, j_fin, k_fin = location\n",
    "        window = self.array[i_ini:i_fin, j_ini:j_fin, k_ini:k_fin]\n",
    "        window = window[np.newaxis, ...]  # add channels dimension\n",
    "        sample = dict(\n",
    "            image=window,\n",
    "            location=location,\n",
    "        )\n",
    "        return sample\n",
    "\n",
    "    @staticmethod\n",
    "    def _enumerate_step_points(starting, ending, win_size, step_size):\n",
    "        starting = max(int(starting), 0)\n",
    "        ending = max(int(ending), 0)\n",
    "        win_size = max(int(win_size), 1)\n",
    "        step_size = max(int(step_size), 1)\n",
    "        if starting > ending:\n",
    "            starting, ending = ending, starting\n",
    "        sampling_point_set = []\n",
    "        while (starting + win_size) <= ending:\n",
    "            sampling_point_set.append(starting)\n",
    "            starting = starting + step_size\n",
    "        additional_last_point = ending - win_size\n",
    "        sampling_point_set.append(max(additional_last_point, 0))\n",
    "        sampling_point_set = np.unique(sampling_point_set).flatten()\n",
    "        if len(sampling_point_set) == 2:\n",
    "            sampling_point_set = np.append(\n",
    "                sampling_point_set, np.round(np.mean(sampling_point_set)))\n",
    "        _, uniq_idx = np.unique(sampling_point_set, return_index=True)\n",
    "        return sampling_point_set[np.sort(uniq_idx)]\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_spatial_coordinates(array, window_shape, border):\n",
    "        shape = array.shape\n",
    "        num_dims = len(shape)\n",
    "        grid_size = [\n",
    "            max(win_size - 2 * border, 0)\n",
    "            for (win_size, border)\n",
    "            in zip(window_shape, border)\n",
    "        ]\n",
    "        steps_along_each_dim = [\n",
    "            GridSampler._enumerate_step_points(\n",
    "                starting=0,\n",
    "                ending=shape[i],\n",
    "                win_size=window_shape[i],\n",
    "                step_size=grid_size[i],\n",
    "            )\n",
    "            for i in range(num_dims)\n",
    "        ]\n",
    "        starting_coords = np.asanyarray(np.meshgrid(*steps_along_each_dim))\n",
    "        starting_coords = starting_coords.reshape((num_dims, -1)).T\n",
    "        n_locations = starting_coords.shape[0]\n",
    "        # prepare the output coordinates matrix\n",
    "        spatial_coords = np.zeros((n_locations, num_dims * 2), dtype=np.int32)\n",
    "        spatial_coords[:, :num_dims] = starting_coords\n",
    "        for idx in range(num_dims):\n",
    "            spatial_coords[:, num_dims + idx] = (\n",
    "                starting_coords[:, idx]\n",
    "                + window_shape[idx]\n",
    "            )\n",
    "        max_coordinates = np.max(spatial_coords, axis=0)[num_dims:]\n",
    "        assert np.all(max_coordinates <= shape[:num_dims]), \\\n",
    "            \"window size greater than the spatial coordinates {} : {}\".format(\n",
    "                max_coordinates, shape)\n",
    "        return spatial_coords\n",
    "\n",
    "\n",
    "class GridAggregator:\n",
    "    \"\"\"\n",
    "    Adapted from NiftyNet\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window_border):\n",
    "        self.window_border = window_border\n",
    "        self.output_array = np.full(\n",
    "            data.shape,\n",
    "            fill_value=0,\n",
    "            dtype=np.uint16,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def crop_batch(windows, location, border=None):\n",
    "        if not border:\n",
    "            return windows, location\n",
    "        location = location.astype(np.int)\n",
    "        batch_shape = windows.shape\n",
    "        spatial_shape = batch_shape[2:]  # ignore batch and channels dim\n",
    "        num_dimensions = 3\n",
    "        for idx in range(num_dimensions):\n",
    "            location[:, idx] = location[:, idx] + border[idx]\n",
    "            location[:, idx + 3] = location[:, idx + 3] - border[idx]\n",
    "        if np.any(location < 0):\n",
    "            return windows, location\n",
    "\n",
    "        cropped_shape = np.max(location[:, 3:6] - location[:, 0:3], axis=0)\n",
    "        diff = spatial_shape - cropped_shape\n",
    "        left = np.floor(diff / 2).astype(np.int)\n",
    "        i_ini, j_ini, k_ini = left\n",
    "        i_fin, j_fin, k_fin = left + cropped_shape\n",
    "        if np.any(left < 0):\n",
    "            raise ValueError\n",
    "        batch = windows[\n",
    "            :,  # batch dimension\n",
    "            :,  # channels dimension\n",
    "            i_ini:i_fin,\n",
    "            j_ini:j_fin,\n",
    "            k_ini:k_fin,\n",
    "        ]\n",
    "        return batch, location\n",
    "\n",
    "    def add_batch(self, windows, locations):\n",
    "        windows = windows.cpu()\n",
    "        location_init = np.copy(locations)\n",
    "        init_ones = np.ones_like(windows)\n",
    "        windows, _ = self.crop_batch(\n",
    "            windows, location_init,\n",
    "            self.window_border,\n",
    "        )\n",
    "        location_init = np.copy(locations)\n",
    "        _, locations = self.crop_batch(\n",
    "            init_ones,\n",
    "            location_init,\n",
    "            self.window_border,\n",
    "        )\n",
    "        for window, location in zip(windows, locations):\n",
    "            window = window.squeeze()\n",
    "            i_ini, j_ini, k_ini, i_fin, j_fin, k_fin = location\n",
    "            self.output_array[i_ini:i_fin, j_ini:j_fin, k_ini:k_fin] = window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42c930-f6b1-43b8-a2ce-44aa6ede6aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50dfa56f-3d95-4ec5-a8ee-e3b000b60ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "# from .sampling import GridSampler, GridAggregator\n",
    "# from .preprocessing import (\n",
    "#     crop,\n",
    "#     preprocess,\n",
    "#     resample_ras_1mm_iso,\n",
    "#     resample_to_reference,\n",
    "#     mean_plus,\n",
    "# )\n",
    "\n",
    "def infer(\n",
    "        input_path,\n",
    "        output_path,\n",
    "        batch_size,\n",
    "        window_cropping,\n",
    "        volume_padding,\n",
    "        window_size,\n",
    "        cuda_device,\n",
    "        use_niftynet_hist_std=False,\n",
    "        ):\n",
    "    # Read image\n",
    "    nii = nib.load(str(input_path))\n",
    "    needs_resampling = check_header(nii)\n",
    "    if needs_resampling:\n",
    "        nii = resample_ras_1mm_iso(nii)\n",
    "    data = nii.get_fdata()\n",
    "\n",
    "    # Preprocessing\n",
    "    hist_masking_function = mean_plus if use_niftynet_hist_std else None\n",
    "    preprocessed = preprocess(\n",
    "        data,\n",
    "        volume_padding,\n",
    "        hist_masking_function=hist_masking_function,\n",
    "    )\n",
    "\n",
    "    # Inference\n",
    "    labels = run_inference(\n",
    "        preprocessed,\n",
    "        get_model(),\n",
    "        window_size,\n",
    "        window_border=window_cropping,\n",
    "        batch_size=batch_size,\n",
    "        cuda_device=cuda_device,\n",
    "    )\n",
    "\n",
    "    # Postprocessing\n",
    "    if volume_padding:\n",
    "        labels = crop(labels, volume_padding)\n",
    "    nib.Nifti1Image(labels, nii.affine).to_filename(str(output_path))\n",
    "\n",
    "    # Resample parcellation to original dimensions\n",
    "    if needs_resampling:\n",
    "        resample_to_reference(\n",
    "            reference_path=input_path,\n",
    "            floating_path=output_path,\n",
    "            result_path=output_path,\n",
    "        )\n",
    "\n",
    "\n",
    "def run_inference(\n",
    "        data,\n",
    "        model,\n",
    "        window_size,\n",
    "        window_border=0,\n",
    "        batch_size=2,\n",
    "        cuda_device=0,\n",
    "        ):\n",
    "    success = False\n",
    "    while not success:\n",
    "        window_sizes = to_tuple(window_size)\n",
    "        window_border = to_tuple(window_border)\n",
    "\n",
    "        sampler = GridSampler(data, window_sizes, window_border)\n",
    "        aggregator = GridAggregator(data, window_border)\n",
    "        loader = DataLoader(sampler, batch_size=batch_size)\n",
    "\n",
    "        device = get_device(cuda_device=cuda_device)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        CHANNELS_DIMENSION = 1\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(loader):\n",
    "                    input_tensor = batch['image'].to(device)\n",
    "                    locations = batch['location']\n",
    "                    logits = model(input_tensor)\n",
    "                    labels = logits.argmax(dim=CHANNELS_DIMENSION, keepdim=True)\n",
    "                    outputs = labels\n",
    "                    aggregator.add_batch(outputs, locations)\n",
    "            success = True\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            print('Window size', window_size, 'is too large.')\n",
    "            window_size = int(window_size * 0.75)\n",
    "            print('Trying with smaller window size', window_size)\n",
    "\n",
    "    return aggregator.output_array\n",
    "\n",
    "\n",
    "def check_header(nifti_image):\n",
    "    orientation = ''.join(nib.aff2axcodes(nifti_image.affine))\n",
    "    spacing = nifti_image.header.get_zooms()[:3]\n",
    "    one_iso = 1, 1, 1\n",
    "    is_ras = orientation == 'RAS'\n",
    "    if not is_ras:\n",
    "        print(f'Detected orientation: {orientation}. Reorienting to RAS...')\n",
    "    is_1_iso = np.allclose(spacing, one_iso)\n",
    "    if not is_1_iso:\n",
    "        print(f'Detected spacing: {spacing}. Resampling to 1 mm iso...')\n",
    "    needs_resampling = not is_ras or not is_1_iso\n",
    "    return needs_resampling\n",
    "\n",
    "\n",
    "def get_device(cuda_device=0):\n",
    "    return torch.device(\n",
    "        'cuda:{}'.format(cuda_device) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def to_tuple(value):\n",
    "    try:\n",
    "        iter(value)\n",
    "    except TypeError:\n",
    "        value = 3 * (value,)\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"\n",
    "    Using PyTorch Hub as I haven't been able to install the .pth file\n",
    "    within the pip package\n",
    "    \"\"\"\n",
    "    repo = 'fepegar/highresnet'\n",
    "    model_name = 'highres3dnet'\n",
    "    model = torch.hub.load(repo, model_name, pretrained=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaa6b5-28de-477e-a171-3462947bed11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ce9a06d-1359-4297-a11a-08d407637a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\dwm\\jupyter_basic\\ch10_monai\\downloaded_data\\data.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from configparser import ConfigParser\n",
    "import os\n",
    "\n",
    "def get_data_url_from_model_zoo():\n",
    "    url = 'https://raw.githubusercontent.com/NifTK/NiftyNetModelZoo/5-reorganising-with-lfs/highres3dnet_brain_parcellation/main.ini'\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        config_string = response.read().decode()\n",
    "    config = ConfigParser()\n",
    "    config.read_string(config_string)\n",
    "    data_url = config['data']['url']\n",
    "    return data_url\n",
    "\n",
    "\n",
    "def download_data(data_url):\n",
    "    # tempdir = Path(tempfile.gettempdir())\n",
    "    tempdir = Path(os.getcwd())\n",
    "    download_dir = tempdir / 'downloaded_data'\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    data_path = download_dir / Path(data_url).name\n",
    "    print(data_path)\n",
    "    if not data_path.is_file():\n",
    "        urllib.request.urlretrieve(data_url, data_path)\n",
    "    with tarfile.open(data_path, 'r') as tar:\n",
    "        tar.extractall(download_dir)\n",
    "    nifti_files = download_dir.glob('**/*.nii.gz')\n",
    "    return list(nifti_files)[0]\n",
    "\n",
    "\n",
    "def test_infer():\n",
    "    image_path = download_data(get_data_url_from_model_zoo())\n",
    "\n",
    "test_infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e366a9b-576b-4153-a43f-a8b6e09fc602",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba00100-7d27-417b-b7de-cdc1b4fef8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    HighRes3DNet by Li et al. 2017 for T1-MRI brain parcellation\n",
      "    pretrained (bool): load parameters from pretrained model\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Daewoon/.cache\\torch\\hub\\fepegar_highresnet_master\n",
      "Using cache found in C:\\Users\\Daewoon/.cache\\torch\\hub\\fepegar_highresnet_master\n"
     ]
    }
   ],
   "source": [
    "repo = 'fepegar/highresnet'\n",
    "model_name = 'highres3dnet'\n",
    "print(torch.hub.help(repo, model_name))\n",
    "\"HighRes3DNet by Li et al. 2017 for T1-MRI brain parcellation\"\n",
    "\"pretrained (bool): load parameters from pretrained model\"\n",
    "model = torch.hub.load(repo, model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835267d8-94aa-4b0a-8c71-fbb45abd2b00",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be363e1b-b063-40c4-a20b-1bd652d8aa1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected orientation: LIA. Reorienting to RAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages\\ipykernel_launcher.py:91: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Using cache found in C:\\Users\\Daewoon/.cache\\torch\\hub\\fepegar_highresnet_master\n",
      "  0%|                                                                                        | 0/27 [00:00<?, ?it/s]d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages\\ipykernel_launcher.py:106: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "d:\\workspace\\dwm\\jupyter_basic\\venv\\lib\\site-packages\\ipykernel_launcher.py:118: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 27/27 [00:16<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# import click\n",
    "import pathlib\n",
    "\n",
    "input_path111 = \"_data/GAAIN/AD01_MR.nii\"\n",
    "input_path211 = \"_data/OASIS/OAS1_MR.nii.gz\"\n",
    "input_path311 = \"_data/SNU/P01_MR.nii\"\n",
    "input_path312 = \"_data/SNU/P01_orig.nii\"\n",
    "input_path321 = \"_data/SNU/P02_mr.nii\"\n",
    "input_path322 = \"_data/SNU/P02_orig.nii\"\n",
    "input_path331 = \"_data/SNU/P03_mr.nii\"\n",
    "input_path332 = \"_data/SNU/P03_orig.nii\"\n",
    "\n",
    "input_path341 = \"_data/SNU/P04_mr.nii\"\n",
    "input_path342 = \"_data/SNU/P04_orig.nii.gz\"\n",
    "input_path361 = \"_data/SNU/P06_mr.nii\"\n",
    "input_path362 = \"_data/SNU/P06_orig.nii\"\n",
    "\n",
    "input_path = input_path342\n",
    "\n",
    "output_path = None\n",
    "if output_path is None:\n",
    "    input_path = pathlib.Path(input_path)\n",
    "    input_name = input_path.name\n",
    "    output_name = input_name.replace('.nii', '_seg.nii')\n",
    "    output_path = input_path.parent / output_name\n",
    "infer(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    batch_size = 1,\n",
    "    window_cropping = 2,\n",
    "    volume_padding = 10,\n",
    "    window_size = 128,\n",
    "    cuda_device = 0,\n",
    "    use_niftynet_hist_std= False,# hist_niftynet,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d05dd554-81b2-4adc-bd79-78aac2839620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942190af-3bc2-4d68-9644-b3e134890439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "77a35ccee26993ab16be666979a7bc0119dbfe0fa7b2d59f42b1045260f18305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
